---
title: "p8105_hw3_jc5924"
output: github_document
name: Sophie Chen
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggridges)
library(patchwork)

library(p8105.datasets)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1
#### Read in the data

```{r}
data("instacart")

instacart = 
  instacart %>% 
  as_tibble(instacart)
```

#### Answer questions about the data

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns, with each row resprenting a single product from an instacart order. Variables include identifiers for user, order, and product; the order in which each product was added to the cart. There are several order-level variables, describing the day and time of the order, and number of days since prior order. Then there are several item-specific variables, describing the product name (e.g. Yogurt, Avocado), department (e.g. dairy and eggs, produce), and aisle (e.g. yogurt, fresh fruits), and whether the item has been ordered by this user in the past. In total, there are `r instacart %>% select(product_id) %>% distinct %>% count` products found in `r instacart %>% select(user_id, order_id) %>% distinct %>% count` orders from `r instacart %>% select(user_id) %>% distinct %>% count` distinct users.

Below is a table summarizing the number of items ordered from aisle. In total, there are 134 aisles, with fresh vegetables and fresh fruits holding the most items ordered by far.

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

Next is a plot that shows the number of items ordered in each aisle. Here, aisles are ordered by ascending number of items.

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))

```

Our next table shows the three most popular items in aisles `baking ingredients`, `dog food care`, and `packaged vegetables fruits`, and includes the number of times each item is ordered in your table.

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```

Finally is a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week. This table has been formatted in an untidy manner for human readers. Pink Lady Apples are generally purchased slightly earlier in the day than Coffee Ice Cream, with the exception of day 5.

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 2)
```

# Problem 2
### Load, tidy and wrangle
```{r}
accel_df=read_csv("./data/accel_data.csv",show_col_types = FALSE) %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to="minute",
    names_prefix="activity_",
    values_to="activity count",
  ) %>%
    mutate(
      weekday_vs_weekend=ifelse(day %in% c("Saturday", "Sunday"), "weekend", "weekday"),
      day = fct_relevel(day,"Monday", "Tuesday", "Wednesday","Thursday","Friday", "Saturday", "Sunday"),
      minute=as.numeric(minute)
      )
accel_df
```
### Describe the resultings
* There are total of `r ncol(accel_df)` key variables in this dataset that are `r names(accel_df)`. And there are total of `r nrow(accel_df)` observations. In this dataframe, variables activity are the activity counts for each minute of a 24-hour day starting at midnight.

### Create a table
```{r}
accel_df %>%
  janitor::clean_names() %>% 
  group_by(week,day) %>%
  summarize(
    total_activity=sum(activity_count,na.rm = TRUE)
) %>% 
  pivot_wider(
    names_from = day,
    values_from = total_activity) %>% 
  knitr::kable()
```

### Are there any trends?
* There are a few trends that can be observed that patients' activity level remains fairly stable for the first three weeks, and for week 4 and week 5, the total activity count drops during the weekends. 

### Single panel plot
```{r}
accel_df %>% 
  janitor::clean_names() %>% 
  group_by(day,minute) %>% 
  ggplot(aes(x = minute/60,y =activity_count,color = day)) + 
  geom_point(alpha = .5) +
  geom_smooth(se = FALSE)+
  labs(
    x = "Hour",
    y = "Activity Counts",
    title = "24-hour activity time courses for each day")+
  scale_x_continuous(
    limits = c(0, 24))
```

### Describe the pattern
* We can see generally from Monday to Sunday, there are not many of activity count toward 00:00am to 6:00am and the most fluctuations occur between 7:00am to 24:00pm. There are two peaks when time are around 8:00am to 15:00pm and 18:00pm to 24:00pm. Plus, we can tell from the plot that from Friday to Sunday, patients tend to be more active than during weekdays.

# Problem 3
### Load and explore

```{r}
data("ny_noaa")
ny_noaa
```

* In this dataset, there are `r ncol(ny_noaa)` key variable that are `r names(ny_noaa)` and there are total of `r nrow(ny_noaa)` of observations. Plus, there is an issue with missing data that `prcp` has `r sum(is.na(ny_noaa$prcp))` missing values; `snow` has `r sum(is.na(ny_noaa$snow))` missing values; `snwd` has `r sum(is.na(ny_noaa$snwd))` missing values; `tmax` has `r sum(is.na(ny_noaa$tmax))` missing values and `tmin` has `r sum(is.na(ny_noaa$tmin))` missing values.

### Tidy, create variables and units
```{r}
noaa_df=ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(date,into = c("year","month","day")) %>%
  mutate(year=as.integer(year),
         month=month.name[as.numeric(month)],
         day=as.numeric(day),
         prcp=as.numeric(prcp)/10,
         tmax=as.numeric(tmax)/10,
         tmin=as.numeric(tmin)/10)
noaa_df
```
### Snowfalls
```{r}
noaa_df %>% 
  janitor::clean_names() %>% 
  group_by(snow) %>%
  summarize(observed_value=n()) %>% 
  arrange(desc(observed_value))
```
* The most commonly observed data is 0 meaning that most of the time, there appears to be no snow in NY. And there are also large number of observed value of missing data for NA.

### Two-panel plot, structure?outliers?
```{r}
avg_max_jan_jul=noaa_df %>% 
  group_by(id,year,month) %>% 
  filter(month %in% c("January", "July")) %>% 
  drop_na(tmax) %>% 
  summarise(avg_max=mean(tmax))

avg_max_jan_jul %>% 
  ggplot(aes(x=year, y=avg_max,color=year))+
  geom_point(alpha=.5)+
  geom_smooth(se=FALSE)+
  facet_grid(.~month)+
  labs(
    x="Year",
    y="Average Max Temperature (C)",
    title = "Average Max Temperature in January and in July in Each Station Across Years"
  )
```

* It is for sure that there is a structure that the avg max temperature in Jan is in deed much lower than that in July. Plus, we can see the huge fluctuation happening in January throughout the years and in July, the flow of avg max temp tend to be stable. There are indeed some outliers both in January and July throughout the years.

### 









